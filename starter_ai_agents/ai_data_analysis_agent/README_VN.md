# üìä AI Data Analysis Agent

·ª®ng d·ª•ng AI Data Analysis Agent ƒë∆∞·ª£c x√¢y d·ª±ng b·∫±ng Agno Agent framework v√† m√¥ h√¨nh OpenAI GPT-4o. Agent n√†y gi√∫p ng∆∞·ªùi d√πng ph√¢n t√≠ch d·ªØ li·ªáu t·ª´ file CSV, Excel th√¥ng qua c√°c c√¢u h·ªèi b·∫±ng ng√¥n ng·ªØ t·ª± nhi√™n, ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi c√°c m√¥ h√¨nh ng√¥n ng·ªØ c·ªßa OpenAI v√† DuckDB ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu hi·ªáu qu·∫£ - gi√∫p vi·ªác ph√¢n t√≠ch d·ªØ li·ªáu tr·ªü n√™n d·ªÖ d√†ng v·ªõi t·∫•t c·∫£ ng∆∞·ªùi d√πng, b·∫•t k·ªÉ h·ªç c√≥ ki·∫øn th·ª©c SQL hay kh√¥ng.

## üéØ Ki·∫øn Th·ª©c ƒê√£ H·ªçc ƒê∆∞·ª£c

### **1. Ki·∫øn Th·ª©c T·ªïng Qu√°t**

#### **Ki·∫øn Th·ª©c Ch√≠nh T·ª´ D·ª± √Ån N√†y:**

**A. Natural Language to SQL (NL2SQL) Pattern**

- **Kh√°i ni·ªám**: S·ª≠ d·ª•ng AI ƒë·ªÉ chuy·ªÉn ƒë·ªïi c√¢u h·ªèi t·ª± nhi√™n th√†nh SQL queries
- **L·ª£i √≠ch**: C√° nh√¢n h√≥a vi·ªác ph√¢n t√≠ch d·ªØ li·ªáu, kh√¥ng c·∫ßn chuy√™n m√¥n SQL
- **Th√°ch th·ª©c**: ƒê·∫£m b·∫£o AI hi·ªÉu ƒë√∫ng schema v√† intent c·ªßa c√¢u h·ªèi
- **Gi·∫£i ph√°p**: S·ª≠ d·ª•ng GPT-4 v·ªõi context v·ªÅ schema d·ªØ li·ªáu

**B. Data Processing Pipeline**

- **Input Processing**: Handle CSV/Excel files, detect data types automatically
- **Data Cleaning**: X·ª≠ l√Ω missing values, type conversion, data validation
- **Schema Inference**: T·ª± ƒë·ªông hi·ªÉu c·∫•u tr√∫c d·ªØ li·ªáu ƒë·ªÉ t·∫°o semantic model
- **Temporary Storage**: S·ª≠ d·ª•ng tempfile ƒë·ªÉ DuckDB c√≥ th·ªÉ query d·ªØ li·ªáu

**C. DuckDB Integration v·ªõi AI**

- **DuckDB**: Database OLAP nhanh, in-process, kh√¥ng c·∫ßn server
- **Semantic Models**: Cung c·∫•p context v·ªÅ tables v√† columns cho AI
- **Query Generation**: AI t·∫°o SQL queries d·ª±a tr√™n semantic model
- **Result Processing**: X·ª≠ l√Ω k·∫øt qu·∫£ v√† format output cho user

#### **So S√°nh V·ªõi C√°c D·ª± √Ån Tr∆∞·ªõc:**

| D·ª± √Ån                | Pattern                            | AI Role              | Output                 |
| -------------------- | ---------------------------------- | -------------------- | ---------------------- |
| **Travel Agent**     | Multi-Agent (Researcher + Planner) | 2 agents ph·ªëi h·ª£p    | L·ªãch tr√¨nh du l·ªãch     |
| **Blog to Podcast**  | Single-Agent + Multi-Tools         | 1 agent ƒëa tools     | Audio content          |
| **Breakup Recovery** | Multi-Agent Emotional              | 4 specialized agents | Emotional support      |
| **Data Analysis**    | NL2SQL Agent                       | 1 agent + DuckDB     | SQL queries & insights |

### **2. Ki·∫øn Th·ª©c T·ªïng Qu√°t**

#### **A. Data Analysis v·ªõi AI Agents**

- **Kh√°i ni·ªám**: S·ª≠ d·ª•ng AI ƒë·ªÉ chuy·ªÉn ƒë·ªïi ng√¥n ng·ªØ t·ª± nhi√™n th√†nh SQL queries
- **L·ª£i √≠ch**: C√° nh√¢n h√≥a vi·ªác ph√¢n t√≠ch d·ªØ li·ªáu, kh√¥ng c·∫ßn chuy√™n m√¥n SQL
- **Pattern**: `Natural Language ‚Üí AI Agent ‚Üí SQL Query ‚Üí Data Analysis ‚Üí Results`

#### **B. DuckDB Integration**

- **V·∫•n ƒë·ªÅ**: C·∫ßn engine database hi·ªáu qu·∫£ ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn
- **DuckDB**: Database OLAP nhanh, in-process, kh√¥ng c·∫ßn server
- **Gi·∫£i ph√°p**: T√≠ch h·ª£p DuckDB v·ªõi AI agents ƒë·ªÉ query d·ªØ li·ªáu hi·ªáu qu·∫£

#### **C. File Processing Pipeline**

- **Input Processing**: X·ª≠ l√Ω CSV/Excel files, detect data types
- **Data Cleaning**: Handle missing values, type conversion
- **Schema Inference**: T·ª± ƒë·ªông hi·ªÉu c·∫•u tr√∫c d·ªØ li·ªáu
- **Temporary Storage**: T·∫°o temporary files ƒë·ªÉ DuckDB c√≥ th·ªÉ query

#### **D. Natural Language to SQL Translation**

- **NL2SQL Challenge**: Chuy·ªÉn ƒë·ªïi c√¢u h·ªèi t·ª± nhi√™n th√†nh SQL queries ch√≠nh x√°c
- **Context Awareness**: AI hi·ªÉu schema v√† relationships trong data
- **Query Optimization**: T·∫°o SQL queries hi·ªáu qu·∫£ v√† ch√≠nh x√°c

### **2. V√≠ D·ª• T·ª´ Code**

#### **A. File Upload & Preprocessing Pipeline**

```python
def preprocess_and_save(file):
    # Read uploaded file into DataFrame
    if file.name.endswith('.csv'):
        df = pd.read_csv(file, encoding='utf-8', na_values=['NA', 'N/A', 'missing'])
    elif file.name.endswith('.xlsx'):
        df = pd.read_excel(file, na_values=['NA', 'N/A', 'missing'])

    # Data type inference and conversion
    for col in df.columns:
        if 'date' in col.lower():
            df[col] = pd.to_datetime(df[col], errors='coerce')
        elif df[col].dtype == 'object':
            try:
                df[col] = pd.to_numeric(df[col])
            except (ValueError, TypeError):
                pass  # Keep as string if conversion fails

    # Save to temporary CSV for DuckDB
    with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as temp_file:
        df.to_csv(temp_path, index=False, quoting=csv.QUOTE_ALL)

    return temp_path, df.columns.tolist(), df
```

#### **B. DuckDB Agent Configuration**

````python
# Semantic model for DuckDB
semantic_model = {
    "tables": [
        {
            "name": "uploaded_data",
            "description": "Contains the uploaded dataset.",
            "path": temp_path,
        }
    ]
}

# Initialize DuckDbAgent
duckdb_agent = DuckDbAgent(
    model=OpenAIChat(model="gpt-4", api_key=api_key),
    semantic_model=json.dumps(semantic_model),
    tools=[PandasTools()],
    markdown=True,
    system_prompt="You are an expert data analyst. Generate SQL queries to solve the user's query. Return only the SQL query, enclosed in ```sql ``` and give the final answer.",
)
````

#### **C. Query Processing Flow**

```python
# Natural language query processing
user_query = st.text_area("Ask a query about the data:")

if st.button("Submit Query"):
    with st.spinner('Processing your query...'):
        # AI converts natural language to SQL
        response = duckdb_agent.run(user_query)

        # Display results
        st.markdown(response.content)
```

### **3. T·ªïng H·ª£p C√°ch S·ª≠ D·ª•ng Cho Sau N√†y**

#### **A. Data Analysis Agent Template**

````python
def create_data_analysis_agent(file_path, api_key):
    # 1. Preprocess data
    temp_path, columns, df = preprocess_and_save(uploaded_file)

    # 2. Create semantic model
    semantic_model = {
        "tables": [{
            "name": "data_table",
            "description": "User uploaded dataset",
            "path": temp_path,
        }]
    }

    # 3. Initialize agent
    agent = DuckDbAgent(
        model=OpenAIChat(model="gpt-4", api_key=api_key),
        semantic_model=json.dumps(semantic_model),
        tools=[PandasTools()],
        system_prompt="""You are an expert data analyst.
        Generate SQL queries to solve user queries.
        Return only the SQL query in ```sql``` blocks and final answer.""",
    )

    return agent, df

# Usage
agent, df = create_data_analysis_agent(file_path, api_key)
response = agent.run("What are the top 5 sales by region?")
````

#### **B. File Processing Best Practices**

```python
def robust_file_processing(file):
    try:
        # 1. Detect file type
        file_ext = file.name.split('.')[-1].lower()

        # 2. Read with appropriate method
        if file_ext == 'csv':
            df = pd.read_csv(file, encoding='utf-8')
        elif file_ext in ['xlsx', 'xls']:
            df = pd.read_excel(file)
        else:
            raise ValueError("Unsupported format")

        # 3. Data cleaning
        df = df.dropna(how='all')  # Remove empty rows
        df.columns = df.columns.str.strip()  # Clean column names

        # 4. Type inference
        for col in df.columns:
            if df[col].dtype == 'object':
                # Try date conversion
                if 'date' in col.lower():
                    df[col] = pd.to_datetime(df[col], errors='coerce')
                # Try numeric conversion
                else:
                    df[col] = pd.to_numeric(df[col], errors='ignore')

        # 5. Create temp file for DuckDB
        with tempfile.NamedTemporaryFile(suffix=".csv", delete=False) as temp:
            df.to_csv(temp.name, index=False, quoting=csv.QUOTE_ALL)
            return temp.name, df

    except Exception as e:
        st.error(f"File processing error: {e}")
        return None, None
```

#### **C. Natural Language Query Optimization**

```python
def optimize_nl_query(user_query, df_schema):
    """Optimize natural language queries for better SQL generation"""

    # Add context about available columns
    context = f"""
    Dataset has these columns: {', '.join(df_schema)}
    Available data types: {df.dtypes.to_dict()}

    User query: {user_query}

    Generate SQL query to answer this question.
    """

    return context

# Usage
optimized_query = optimize_nl_query(
    "Show me sales by month",
    df.columns.tolist()
)
response = agent.run(optimized_query)
```

## üéØ H∆∞·ªõng D·∫´n Prompt Engineering Cho Data Analysis

### **1. System Prompt Optimization**

#### **A. Basic vs Advanced System Prompts**

````python
# ‚ùå Too basic
system_prompt = "You are a data analyst."

# ‚úÖ Context-aware and specific
system_prompt = """You are an expert data analyst specializing in SQL query generation.

Given a user query about their dataset, generate precise SQL queries that:
1. Use correct table and column names
2. Apply appropriate aggregations and filters
3. Handle date formatting and data types properly
4. Return results in readable format

Dataset context: {table_schema}
Available columns: {columns_list}
Data types: {data_types}

Return only the SQL query in ```sql``` blocks followed by the final answer."""
````

### **2. Query Enhancement Techniques**

#### **A. Few-Shot Examples**

```python
few_shot_examples = """
Example 1:
User: "What are the total sales by region?"
SQL: SELECT region, SUM(sales) as total_sales FROM uploaded_data GROUP BY region ORDER BY total_sales DESC;

Example 2:
User: "Show me customers who spent more than $1000"
SQL: SELECT customer_name, total_spent FROM uploaded_data WHERE total_spent > 1000 ORDER BY total_spent DESC;

Example 3:
User: "What's the average order value by month?"
SQL: SELECT strftime('%Y-%m', order_date) as month, AVG(order_value) as avg_value FROM uploaded_data GROUP BY month ORDER BY month;
"""

enhanced_prompt = f"{few_shot_examples}\n\nNow, for this query: {user_query}"
```

## T√≠nh nƒÉng

- üì§ **H·ªó tr·ª£ Upload File**:

  - Upload file CSV v√† Excel
  - T·ª± ƒë·ªông ph√°t hi·ªán ki·ªÉu d·ªØ li·ªáu v√† schema
  - H·ªó tr·ª£ nhi·ªÅu ƒë·ªãnh d·∫°ng file

- üí¨ **Truy v·∫•n Ng√¥n ng·ªØ T·ª± nhi√™n**:

  - Chuy·ªÉn ƒë·ªïi c√¢u h·ªèi t·ª± nhi√™n th√†nh SQL queries
  - Nh·∫≠n c√¢u tr·∫£ l·ªùi t·ª©c th√¨ v·ªÅ d·ªØ li·ªáu
  - Kh√¥ng c·∫ßn ki·∫øn th·ª©c SQL

- üîç **Ph√¢n t√≠ch N√¢ng cao**:

  - Th·ª±c hi·ªán c√°c ph√©p t·ªïng h·ª£p d·ªØ li·ªáu ph·ª©c t·∫°p
  - L·ªçc v√† s·∫Øp x·∫øp d·ªØ li·ªáu
  - T·∫°o b√°o c√°o th·ªëng k√™
  - T·∫°o bi·ªÉu ƒë·ªì v√† visualizations

- üéØ **Giao di·ªán T∆∞∆°ng t√°c**:
  - Giao di·ªán Streamlit th√¢n thi·ªán v·ªõi ng∆∞·ªùi d√πng
  - X·ª≠ l√Ω truy v·∫•n theo th·ªùi gian th·ª±c
  - Tr√¨nh b√†y k·∫øt qu·∫£ r√µ r√†ng

## C√°ch b·∫Øt ƒë·∫ßu?

1. Clone kho l∆∞u tr·ªØ GitHub

```bash
git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git
cd awesome-llm-apps/starter_ai_agents/ai_data_analysis_agent
```

2. C√†i ƒë·∫∑t c√°c dependencies c·∫ßn thi·∫øt:

```bash
pip install -r requirements.txt
```

3. L·∫•y GEMINI API Key c·ªßa b·∫°n

4. Ch·∫°y ·ª©ng d·ª•ng Streamlit

```bash
streamlit run ai_data_analyst.py
```

## C√°ch ho·∫°t ƒë·ªông?

AI Data Analysis Agent ho·∫°t ƒë·ªông theo quy tr√¨nh sau:

### **1. Data Ingestion & Preprocessing**

```python
def preprocess_and_save(file):
    # ƒê·ªçc file v√† detect format
    if file.name.endswith('.csv'):
        df = pd.read_csv(file, encoding='utf-8', na_values=['NA', 'N/A', 'missing'])
    elif file.name.endswith('.xlsx'):
        df = pd.read_excel(file, na_values=['NA', 'N/A', 'missing'])

    # Data cleaning v√† type inference
    for col in df.columns:
        if 'date' in col.lower():
            df[col] = pd.to_datetime(df[col], errors='coerce')
        elif df[col].dtype == 'object':
            try:
                df[col] = pd.to_numeric(df[col])
            except (ValueError, TypeError):
                pass  # Keep as string

    # L∆∞u temporary file cho DuckDB
    with tempfile.NamedTemporaryFile(suffix=".csv", delete=False) as temp_file:
        df.to_csv(temp_path, index=False, quoting=csv.QUOTE_ALL)

    return temp_path, df.columns.tolist(), df
```

### **2. Semantic Model Creation**

```python
semantic_model = {
    "tables": [{
        "name": "uploaded_data",
        "description": "Contains the uploaded dataset.",
        "path": temp_path,
    }]
}
```

### **3. AI Agent Initialization**

```python
duckdb_agent = DuckDbAgent(
    model=OpenAIChat(model="gpt-4", api_key=api_key),
    semantic_model=json.dumps(semantic_model),
    tools=[PandasTools()],
    system_prompt="You are an expert data analyst. Generate SQL queries to solve the user's query.",
)
```

### **4. Query Processing Flow**

```python
# Natural language ‚Üí SQL ‚Üí Results
user_query = "What are the top 5 products by sales?"
response = duckdb_agent.run(user_query)
st.markdown(response.content)
```

## Ki·∫øn tr√∫c k·ªπ thu·∫≠t chi ti·∫øt

### **Core Components:**

1. **Frontend Layer (Streamlit)**

   - File upload interface
   - Query input textarea
   - Results display
   - API key management

2. **Data Processing Layer**

   - File format detection (CSV/Excel)
   - Data type inference
   - Schema extraction
   - Temporary file management

3. **AI Processing Layer**

   - OpenAI GPT-4 integration
   - Natural language understanding
   - SQL query generation
   - Context management

4. **Database Layer (DuckDB)**
   - In-memory data processing
   - SQL query execution
   - Result formatting
   - Performance optimization

### **Data Flow:**

```
User Upload ‚Üí File Processing ‚Üí Schema Inference ‚Üí Semantic Model
                                        ‚Üì
Natural Language Query ‚Üí GPT-4 ‚Üí SQL Generation ‚Üí DuckDB ‚Üí Results ‚Üí UI Display
```

## V√≠ d·ª• s·ª≠ d·ª•ng

### **D·ªØ li·ªáu m·∫´u**: File CSV nh√¢n vi√™n v·ªõi c√°c c·ªôt: name, age, city, salary, department

### **C√°c c√¢u h·ªèi c√≥ th·ªÉ h·ªèi**:

- "Tu·ªïi trung b√¨nh c·ªßa nh√¢n vi√™n theo ph√≤ng ban?"
- "Th√†nh ph·ªë n√†o c√≥ nhi·ªÅu nh√¢n vi√™n nh·∫•t?"
- "Top 3 ph√≤ng ban c√≥ l∆∞∆°ng cao nh·∫•t?"
- "Ph√¢n b·ªë l∆∞∆°ng theo ƒë·ªô tu·ªïi?"
- "T·ªïng l∆∞∆°ng theo khu v·ª±c ƒë·ªãa l√Ω?"

### **K·∫øt qu·∫£ m·∫´u**:

**Query**: "Tu·ªïi trung b√¨nh theo ph√≤ng ban?"

```sql
SELECT department, AVG(age) as avg_age
FROM uploaded_data
GROUP BY department
ORDER BY avg_age DESC;
```

**K·∫øt qu·∫£**:
| department | avg_age |
|------------|---------|
| Sales | 40.0 |
| HR | 36.0 |
| Marketing | 29.5 |
| Engineering| 28.5 |

## Debug v√† Troubleshooting

### **C√°c l·ªói th∆∞·ªùng g·∫∑p:**

1. **API Key Issues**

   ```python
   # Error: "Please enter your OpenAI API key to proceed."
   # Solution: Enter valid OpenAI API key in sidebar
   ```

2. **File Processing Errors**

   ```python
   # Error: "Unsupported file format"
   # Solution: Only upload CSV or Excel files
   ```

3. **Data Type Issues**

   ```python
   # Error: "Error processing file: ..."
   # Solution: Check file encoding, missing values, column names
   ```

4. **Query Generation Issues**
   ```python
   # Error: "No results found"
   # Solution: Rephrase query, check column names, verify data
   ```

### **Best Practices cho Debugging:**

1. **Check Data First**: Lu√¥n xem preview data tr∆∞·ªõc khi query
2. **Simple Queries First**: B·∫Øt ƒë·∫ßu v·ªõi queries ƒë∆°n gi·∫£n
3. **Verify Column Names**: ƒê·∫£m b·∫£o t√™n c·ªôt ch√≠nh x√°c
4. **Check Data Types**: Verify numeric/date columns ƒë∆∞·ª£c detect ƒë√∫ng
5. **Monitor Terminal**: Xem output trong terminal cho debugging info

## Ki·∫øn tr√∫c k·ªπ thu·∫≠t

- **Frontend**: Streamlit cho giao di·ªán web
- **AI Engine**: OpenAI GPT-4o cho NL2SQL conversion
- **Database**: DuckDB cho data processing v√† querying
- **Data Processing**: Pandas cho file I/O v√† preprocessing
- **Tools**: Agno framework cho agent orchestration

## Best Practices ƒë√£ h·ªçc

1. **Data Validation**: Lu√¥n validate input data tr∆∞·ªõc khi x·ª≠ l√Ω
2. **Error Handling**: Implement comprehensive error handling cho user experience t·ªët
3. **Type Inference**: T·ª± ƒë·ªông detect v√† convert data types
4. **Query Optimization**: S·ª≠ d·ª•ng semantic models ƒë·ªÉ c·∫£i thi·ªán accuracy
5. **User Feedback**: Cung c·∫•p progress indicators v√† clear error messages
6. **Security**: Handle API keys an to√†n, validate file uploads
7. **Performance**: S·ª≠ d·ª•ng temporary files v√† efficient data structures

---

## üìö **T·ªîNG K·∫æT KI·∫æN TH·ª®C ƒê√É H·ªåC**

### **1. AI Data Analysis Agent - Ki·∫øn Th·ª©c M·ªõi**

#### **A. Natural Language to SQL (NL2SQL)**

- **Pattern m·ªõi**: Single-Agent v·ªõi DuckDB integration
- **Th√°ch th·ª©c**: Chuy·ªÉn ƒë·ªïi ng√¥n ng·ªØ t·ª± nhi√™n th√†nh SQL queries ch√≠nh x√°c
- **Gi·∫£i ph√°p**: GPT-4 + semantic models + context injection

#### **B. Data Processing Pipeline**

- **File Handling**: CSV/Excel auto-detection
- **Data Cleaning**: Missing values, type conversion, schema inference
- **Temporary Storage**: tempfile management cho DuckDB

#### **C. DuckDB Integration**

- **In-Process Database**: Kh√¥ng c·∫ßn server setup
- **High Performance**: OLAP queries tr√™n large datasets
- **SQL Generation**: AI t·∫°o queries d·ª±a tr√™n user intent

### **2. Pattern Comparison v·ªõi D·ª± √Ån Tr∆∞·ªõc**

| Aspect           | Travel Agent       | Blog‚ÜíPodcast          | Breakup Recovery       | **Data Analysis**          |
| ---------------- | ------------------ | --------------------- | ---------------------- | -------------------------- |
| **Architecture** | Multi-Agent        | Single + Tools        | Multi-Agent Emotional  | **NL2SQL Agent**           |
| **AI Focus**     | Content Generation | Multi-Modal           | Emotional Intelligence | **Data Intelligence**      |
| **Tools**        | SerpAPI, Groq      | Firecrawl, ElevenLabs | Gemini Vision          | **DuckDB, Pandas**         |
| **Output**       | Travel Itinerary   | Audio Content         | Emotional Support      | **SQL Queries & Insights** |
| **Complexity**   | Medium             | Medium                | High                   | **Medium-High**            |

### **3. Technical Skills Acquired**

#### **A. Data Engineering Skills**

- **File Processing**: CSV/Excel handling v·ªõi pandas
- **Data Type Inference**: Automatic column type detection
- **Schema Management**: Semantic model creation
- **Temporary File Management**: Secure temp file handling

#### **B. AI Integration Skills**

- **NL2SQL**: Natural language to SQL conversion
- **Context Management**: Providing schema context to AI
- **Prompt Engineering**: System prompts cho data analysis
- **Error Handling**: Robust error handling cho AI responses

#### **C. Database Skills**

- **DuckDB**: In-memory analytical database
- **SQL Generation**: AI-powered SQL query creation
- **Result Processing**: Formatting v√† displaying results
- **Performance Optimization**: Efficient data processing

### **4. Best Practices Learned**

#### **A. Data Analysis Best Practices**

```python
# ‚úÖ Good: Comprehensive data preprocessing
def preprocess_data(file):
    df = load_file(file)
    df = clean_data(df)
    df = infer_types(df)
    return df

# ‚úÖ Good: Semantic model for AI context
semantic_model = {
    "tables": [{
        "name": "data",
        "description": "User dataset",
        "path": temp_path
    }]
}
```

#### **B. User Experience Best Practices**

```python
# ‚úÖ Good: Clear user feedback
with st.spinner('Processing your query...'):
    result = agent.run(query)

# ‚úÖ Good: Data preview before analysis
st.write("Uploaded Data:")
st.dataframe(df.head())

# ‚úÖ Good: Error handling with user-friendly messages
try:
    result = process_data(data)
except Exception as e:
    st.error(f"Processing failed: {str(e)}")
```

### **5. Template Patterns Cho D·ª± √Ån T∆∞∆°ng Lai**

#### **A. Data Analysis Agent Template**

```python
def create_data_agent_template():
    return {
        "components": {
            "data_processor": "Pandas-based file processing",
            "ai_engine": "GPT-4 with semantic models",
            "database": "DuckDB for querying",
            "frontend": "Streamlit for UI"
        },
        "patterns": {
            "nl2sql": "Natural language to SQL conversion",
            "semantic_model": "Context provision for AI",
            "error_handling": "Comprehensive exception handling"
        }
    }
```

#### **B. Reusable Components**

```python
class DataAnalysisAgent:
    def __init__(self, api_key):
        self.api_key = api_key
        self.semantic_model = None

    def process_file(self, file):
        # File processing logic
        pass

    def create_semantic_model(self, df, temp_path):
        # Semantic model creation
        pass

    def query_data(self, natural_language_query):
        # NL2SQL processing
        pass
```

**Ki·∫øn th·ª©c t√≠ch l≈©y:**

1. Multi-Agent Architecture (Travel Agent)
2. Single-Agent Multi-Tools (Blog‚ÜíPodcast)
3. Emotional AI & Vision (Breakup Recovery)
4. **NL2SQL & Data Processing (Data Analysis)**
